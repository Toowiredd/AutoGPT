{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 9071211,
          "sourceType": "datasetVersion",
          "datasetId": 5471566
        },
        {
          "sourceId": 8968850,
          "sourceType": "datasetVersion",
          "datasetId": 5399261
        },
        {
          "sourceId": 88967,
          "sourceType": "modelInstanceVersion",
          "isSourceIdPinned": true,
          "modelInstanceId": 74641,
          "modelId": 99402
        }
      ],
      "dockerImageVersionId": 30747,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Toowiredd/AutoGPT/blob/master/Copy_of_yolov8trainingfromvideo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'cdsset:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F5399261%2F8968850%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240811%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240811T010654Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D8c47ed80b4c29ed8f1727e822297ba24c9fee0eb586f9ed6d94cf045d22a0d3186c61a16f57843739975a5625402d72caa6431848acec75f08cdc0907ea4d969d2ce8594ad8f5dd984fbe3dd98b58eb1cb04be3f2dd704571405e984b92b782c6730a19980f5c2ae9ac5d851902a95cfc211e660632ff5cf12d9953a9cb6a57943f6223428d71a29f5ae0ca713d611d0ab7be43838154a826e5436575d00d5370a98bb5d8e8d14b9fd8125546290f2081679988f8a92a5ceea8705b44cc322680e277f3ea50a1f78864f8d72041e8ee05bdb2205de263f337100bd5cb9e483c82fb1f2b995999fff4717ec52e71a79e07d208480c6fca702407deb03a70a7294,bottle-videos:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F5471566%2F9071211%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240811%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240811T010654Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D77fc511f82cc155778b6de92feee22740348cddce5d02c12db03a2ab7afbe7c4170878489a6d905f61d9a21cd4d22b54b70752a010f98370a9f22cd94d676db88bc391d126efb89d39038c7db48a004e1182aad3e2d7506a85c61cf75565640ff562adafe83dc13b466cac5aca229fdcd77ff431fba550e4c1ec0df48f3d446f9ca889eafe534e9f20921826446c0afbe8b51c2d4244603d2e719f97be34e852e0887154ea46b8ba8467d60ff99f84c29ab825a8b9294b0f396cece06252f74f275871aeee8297d500541ad5f0cf82df6aeca0c2a15d70fb340e2b72c2283dee68472f2f18e9fa21a71e05a47b37cb7acb2c05b09b40c682a23cfee2ded775b7,yolov8/tflite/yolov8-bottlesncans/1:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-models-data%2F74641%2F88967%2Fbundle%2Farchive.tar.gz%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240811%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240811T010654Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D6e043ca2bb967f7a10489d585a2ffc280ea1a17bbe06a3e406eaa52044d8f0a0874c45a3f08f4ef2ae38357aefb08d07be5a35d2e115f0e8a36a997cf225bdcda1addc91236a1231690958c9de91ac0bfeb0da563edc33f45b7199a2800b7e24cf4147e99edada51552a01254c5ad90a5c239064dbd527e157e2b1c005eed15bcc7b8d93e5a11606c19e5c1b177d472d2cd396cb7db1ddbdfe9b409a1e73d0bc576822c46477b97da31d698db01f7b328faf48cf6e8e0bfc97a5e93c17f62851606eb6f47b4159964e69b967e708dd0b64137d40725615ed5213388a45cdf795a8aeb67d2fbd3b92176ae241bc4da1499ef6a05ab051018d6374cb2e9e7a9d68'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "Sw_MBjii-WQO"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "TRAINING YOLOV8 FOR DRINK CONTAINER DETECTI0N"
      ],
      "metadata": {
        "id": "KG0XdFmk-WQQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up the environment and import libraries\n"
      ],
      "metadata": {
        "id": "h3hsgLrN-WQS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics\n",
        "import torch\n",
        "import torch.distributed as dist\n",
        "import torch.multiprocessing as mp\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "import torchvision\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import shutil\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights\n",
        "from torchvision.transforms import functional as F\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "import logging\n",
        "import argparse\n",
        "from ultralytics import YOLO\n",
        "import albumentations as A\n",
        "import yaml\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-11T00:59:07.173788Z",
          "iopub.execute_input": "2024-08-11T00:59:07.174428Z",
          "iopub.status.idle": "2024-08-11T00:59:07.2115Z",
          "shell.execute_reply.started": "2024-08-11T00:59:07.174396Z",
          "shell.execute_reply": "2024-08-11T00:59:07.210189Z"
        },
        "trusted": true,
        "id": "ejy8sViR-WQS",
        "outputId": "05044081-1bd8-4fa0-b38c-5d753572a61b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.2.75-py3-none-any.whl.metadata (41 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/41.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.26.4)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.10.0.84)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.13.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.3.1+cu121)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.18.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.1.4)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.1)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2024.7.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2024.6.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Downloading ultralytics-8.2.75-py3-none-any.whl (865 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m865.6/865.6 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Downloading ultralytics_thop-2.0.0-py3-none-any.whl (25 kB)\n",
            "Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, ultralytics-thop, ultralytics\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105 ultralytics-8.2.75 ultralytics-thop-2.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Initialization\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0ABTfJFu-WQS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_object_detection_model(local_rank, model_type='yolov8'):\n",
        "    device = torch.device(f\"cuda:{local_rank}\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    if model_type == 'yolov8':\n",
        "        model = YOLO('yolov8n.pt')  # Load a pretrained YOLOv8 model\n",
        "    else:\n",
        "        # Fallback to Faster R-CNN\n",
        "        model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    if torch.cuda.device_count() > 1:\n",
        "        model = DDP(model, device_ids=[local_rank])\n",
        "\n",
        "    return model, model.names if model_type == 'yolov8' else FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT.meta[\"categories\"]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-11T00:55:24.237654Z",
          "iopub.status.idle": "2024-08-11T00:55:24.23797Z",
          "shell.execute_reply.started": "2024-08-11T00:55:24.237814Z",
          "shell.execute_reply": "2024-08-11T00:55:24.237828Z"
        },
        "trusted": true,
        "id": "C0Git1X0-WQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Augmentation"
      ],
      "metadata": {
        "id": "0BPAgzOU-WQT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_augmentation():\n",
        "    return A.Compose([\n",
        "        A.RandomBrightnessContrast(p=0.5),\n",
        "        A.HueSaturationValue(p=0.5),\n",
        "        A.RandomRotate90(p=0.5),\n",
        "        A.Flip(p=0.5),\n",
        "        A.Blur(blur_limit=3, p=0.3),\n",
        "    ], bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels']))\n",
        "\n",
        "augmentation = get_augmentation()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-11T00:55:24.238812Z",
          "iopub.status.idle": "2024-08-11T00:55:24.239193Z",
          "shell.execute_reply.started": "2024-08-11T00:55:24.239009Z",
          "shell.execute_reply": "2024-08-11T00:55:24.239024Z"
        },
        "trusted": true,
        "id": "xCYEYs9H-WQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Video Processing with Augmentation"
      ],
      "metadata": {
        "id": "8L8pjzqo-WQT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_video(video_file, model, categories, user_labels, base_dir, local_rank, max_frames):\n",
        "    device = torch.device(f\"cuda:{local_rank}\" if torch.cuda.is_available() else \"cpu\")\n",
        "    cap = cv2.VideoCapture(video_file)\n",
        "\n",
        "    frame_count = 0\n",
        "    while cap.isOpened() and (max_frames is None or frame_count < max_frames):\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        image = Image.fromarray(frame_rgb)\n",
        "\n",
        "        # Detect objects\n",
        "        if isinstance(model, YOLO):\n",
        "            results = model(frame_rgb)\n",
        "            boxes = results[0].boxes.xyxy.cpu().numpy()\n",
        "            labels = results[0].boxes.cls.cpu().numpy()\n",
        "            scores = results[0].boxes.conf.cpu().numpy()\n",
        "        else:\n",
        "            tensor = F.to_tensor(image).unsqueeze(0).to(device)\n",
        "            with torch.no_grad():\n",
        "                prediction = model(tensor)[0]\n",
        "            boxes = prediction['boxes'].cpu().numpy()\n",
        "            labels = prediction['labels'].cpu().numpy()\n",
        "            scores = prediction['scores'].cpu().numpy()\n",
        "\n",
        "        # Apply augmentation\n",
        "        augmented = augmentation(image=np.array(image), bboxes=boxes, class_labels=labels)\n",
        "        augmented_image = Image.fromarray(augmented['image'])\n",
        "        augmented_boxes = np.array(augmented['bboxes'])\n",
        "        augmented_labels = np.array(augmented['class_labels'])\n",
        "\n",
        "        # Label and save original objects\n",
        "        label_and_save_objects(image, boxes, labels, scores, categories, user_labels, base_dir, frame_count, \"original\")\n",
        "\n",
        "        # Label and save augmented objects\n",
        "        label_and_save_objects(augmented_image, augmented_boxes, augmented_labels, scores, categories, user_labels, base_dir, frame_count, \"augmented\")\n",
        "\n",
        "        frame_count += 1\n",
        "\n",
        "    cap.release()\n",
        "    return frame_count"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-11T00:55:24.240834Z",
          "iopub.status.idle": "2024-08-11T00:55:24.241181Z",
          "shell.execute_reply.started": "2024-08-11T00:55:24.24101Z",
          "shell.execute_reply": "2024-08-11T00:55:24.241024Z"
        },
        "trusted": true,
        "id": "Tk8Tky1H-WQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create YAML Configuration"
      ],
      "metadata": {
        "id": "S3np7JSa-WQU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_yaml_config(base_dir, class_names):\n",
        "    config = {\n",
        "        'path': base_dir,\n",
        "        'train': 'images/train',\n",
        "        'val': 'images/val',\n",
        "        'test': 'images/test',\n",
        "        'nc': len(class_names),\n",
        "        'names': class_names\n",
        "    }\n",
        "\n",
        "    with open(os.path.join(base_dir, 'dataset.yaml'), 'w') as f:\n",
        "        yaml.dump(config, f)\n",
        "\n",
        "    logging.info(f\"Created YAML configuration at {os.path.join(base_dir, 'dataset.yaml')}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-11T00:55:24.242733Z",
          "iopub.status.idle": "2024-08-11T00:55:24.243103Z",
          "shell.execute_reply.started": "2024-08-11T00:55:24.242923Z",
          "shell.execute_reply": "2024-08-11T00:55:24.242939Z"
        },
        "trusted": true,
        "id": "JXkVYtHG-WQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split Dataset"
      ],
      "metadata": {
        "id": "2SAko1mJ-WQU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_dataset(base_dir, train_ratio=0.7, val_ratio=0.2, test_ratio=0.1):\n",
        "    image_dir = os.path.join(base_dir, 'images')\n",
        "    label_dir = os.path.join(base_dir, 'labels')\n",
        "\n",
        "    all_images = [f for f in os.listdir(image_dir) if f.endswith(('.jpg', '.png'))]\n",
        "\n",
        "    train_images, temp_images = train_test_split(all_images, train_size=train_ratio, random_state=42)\n",
        "    val_images, test_images = train_test_split(temp_images, train_size=val_ratio/(val_ratio+test_ratio), random_state=42)\n",
        "\n",
        "    for split, images in [('train', train_images), ('val', val_images), ('test', test_images)]:\n",
        "        os.makedirs(os.path.join(image_dir, split), exist_ok=True)\n",
        "        os.makedirs(os.path.join(label_dir, split), exist_ok=True)\n",
        "        for img in images:\n",
        "            shutil.move(os.path.join(image_dir, img), os.path.join(image_dir, split, img))\n",
        "            label = img.rsplit('.', 1)[0] + '.txt'\n",
        "            if os.path.exists(os.path.join(label_dir, label)):\n",
        "                shutil.move(os.path.join(label_dir, label), os.path.join(label_dir, split, label))\n",
        "\n",
        "    logging.info(f\"Dataset split: {len(train_images)} train, {len(val_images)} validation, {len(test_images)} test\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-11T00:55:24.245844Z",
          "iopub.status.idle": "2024-08-11T00:55:24.246182Z",
          "shell.execute_reply.started": "2024-08-11T00:55:24.246009Z",
          "shell.execute_reply": "2024-08-11T00:55:24.246022Z"
        },
        "trusted": true,
        "id": "OeZRor6c-WQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create directories for labeled data\n"
      ],
      "metadata": {
        "id": "xRe_Ze3P-WQU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_directories():\n",
        "    base_dir = \"/kaggle/working/labeled_data\"\n",
        "    dirs = [\n",
        "        f\"{base_dir}/images/train\",\n",
        "        f\"{base_dir}/images/val\",\n",
        "        f\"{base_dir}/labels/train\",\n",
        "        f\"{base_dir}/labels/val\"\n",
        "    ]\n",
        "    for dir in dirs:\n",
        "        os.makedirs(dir, exist_ok=True)\n",
        "    return base_dir\n",
        "\n",
        "base_dir = create_directories()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-11T00:55:24.247568Z",
          "iopub.status.idle": "2024-08-11T00:55:24.247893Z",
          "shell.execute_reply.started": "2024-08-11T00:55:24.247733Z",
          "shell.execute_reply": "2024-08-11T00:55:24.247747Z"
        },
        "trusted": true,
        "id": "T9gI_Q2X-WQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize Labeled Images"
      ],
      "metadata": {
        "id": "Wt7pgxvK-WQU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_labeled_images(base_dir, num_images=5):\n",
        "    image_dir = os.path.join(base_dir, 'images', 'train')\n",
        "    label_dir = os.path.join(base_dir, 'labels', 'train')\n",
        "\n",
        "    images = [f for f in os.listdir(image_dir) if f.endswith(('.jpg', '.png'))]\n",
        "    selected_images = np.random.choice(images, num_images, replace=False)\n",
        "\n",
        "    fig, axs = plt.subplots(1, num_images, figsize=(20, 4))\n",
        "    for i, img_name in enumerate(selected_images):\n",
        "        img_path = os.path.join(image_dir, img_name)\n",
        "        label_path = os.path.join(label_dir, img_name.rsplit('.', 1)[0] + '.txt')\n",
        "\n",
        "        img = Image.open(img_path)\n",
        "        draw = ImageDraw.Draw(img)\n",
        "\n",
        "        if os.path.exists(label_path):\n",
        "            with open(label_path, 'r') as f:\n",
        "                for line in f:\n",
        "                    class_id, x, y, w, h = map(float, line.strip().split())\n",
        "                    left = (x - w/2) * img.width\n",
        "                    top = (y - h/2) * img.height\n",
        "                    right = (x + w/2) * img.width\n",
        "                    bottom = (y + h/2) * img.height\n",
        "                    draw.rectangle([left, top, right, bottom], outline='red', width=2)\n",
        "\n",
        "        axs[i].imshow(img)\n",
        "        axs[i].axis('off')\n",
        "        axs[i].set_title(f'Image {i+1}')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-11T00:55:24.248908Z",
          "iopub.status.idle": "2024-08-11T00:55:24.249261Z",
          "shell.execute_reply.started": "2024-08-11T00:55:24.249072Z",
          "shell.execute_reply": "2024-08-11T00:55:24.249086Z"
        },
        "trusted": true,
        "id": "ayoZUUHs-WQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main Function"
      ],
      "metadata": {
        "id": "m0MlPCqc-WQV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    parser = argparse.ArgumentParser(description=\"YOLOv8 Training for Drink Container Detection\")\n",
        "    parser.add_argument(\"--input_dir\", type=str, required=True, help=\"Input directory containing video files\")\n",
        "    parser.add_argument(\"--output_dir\", type=str, required=True, help=\"Output directory for processed data\")\n",
        "    parser.add_argument(\"--max_frames\", type=int, default=None, help=\"Maximum number of frames to process per video\")\n",
        "    parser.add_argument(\"--num_gpus\", type=int, default=1, help=\"Number of GPUs to use\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "    video_files = [os.path.join(args.input_dir, f) for f in os.listdir(args.input_dir) if f.endswith(('.mp4', '.avi', '.mov'))]\n",
        "    user_labels = get_user_labels()  # Implement this function to get user-defined labels\n",
        "\n",
        "    if args.num_gpus > 1 and torch.cuda.is_available():\n",
        "        world_size = min(args.num_gpus, torch.cuda.device_count())\n",
        "        mp.spawn(distributed_process_videos,\n",
        "                 args=(world_size, video_files, user_labels, args.output_dir, args.max_frames),\n",
        "                 nprocs=world_size,\n",
        "                 join=True)\n",
        "    else:\n",
        "        model, categories = get_object_detection_model(0)\n",
        "        for video_file in tqdm(video_files, desc=\"Processing videos\"):\n",
        "            process_video(video_file, model, categories, user_labels, args.output_dir, 0, args.max_frames)\n",
        "\n",
        "    # Split the dataset\n",
        "    split_dataset(args.output_dir)\n",
        "\n",
        "    # Create YAML configuration\n",
        "    create_yaml_config(args.output_dir, list(user_labels.keys()))\n",
        "\n",
        "    # Visualize some labeled images\n",
        "    visualize_labeled_images(args.output_dir)\n",
        "\n",
        "    logging.info(\"Video processing completed. Dataset created at %s\", args.output_dir)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-11T00:55:24.362575Z",
          "iopub.execute_input": "2024-08-11T00:55:24.362935Z",
          "iopub.status.idle": "2024-08-11T00:55:24.377736Z",
          "shell.execute_reply.started": "2024-08-11T00:55:24.36291Z",
          "shell.execute_reply": "2024-08-11T00:55:24.376532Z"
        },
        "trusted": true,
        "id": "jlW7y-Za-WQV",
        "outputId": "1eb2e335-0377-4833-fa61-ce10410a5583",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "usage: colab_kernel_launcher.py [-h] --input_dir INPUT_DIR --output_dir OUTPUT_DIR\n",
            "                                [--max_frames MAX_FRAMES] [--num_gpus NUM_GPUS]\n",
            "colab_kernel_launcher.py: error: the following arguments are required: --input_dir, --output_dir\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "2",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jbiVBhss-WQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8BoQ2Ipy-WQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0uv6ek4h-WQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load and preprocess the video\n"
      ],
      "metadata": {
        "id": "SbY5fGxT-WQV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#def load_video(video_path):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        print(f\"Error: Could not open video file at {video_path}\")\n",
        "        return []\n",
        "\n",
        "    frames = []\n",
        "    frame_count = 0\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frames.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "        frame_count += 1\n",
        "        if frame_count % 100 == 0:\n",
        "            print(f\"Processed {frame_count} frames\")\n",
        "\n",
        " #   cap.release()\n",
        " #   print(f\"Total frames processed: {len(frames)}\")\n",
        "#    return frames\n",
        "\n",
        "#video_path = \"/kaggle/input/bottle-videos\" # Make sure this path is correct\n",
        "#frames = load_video(video_path)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-11T00:55:24.382672Z",
          "iopub.execute_input": "2024-08-11T00:55:24.382972Z",
          "iopub.status.idle": "2024-08-11T00:55:24.390865Z",
          "shell.execute_reply.started": "2024-08-11T00:55:24.382938Z",
          "shell.execute_reply": "2024-08-11T00:55:24.389266Z"
        },
        "trusted": true,
        "id": "YJE6J8Af-WQV",
        "outputId": "cc75c935-45cc-41bd-8560-040cc75240ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "traceback": [
            "\u001b[0;36m  Cell \u001b[0;32mIn[16], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    cap = cv2.VideoCapture(video_path)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ],
          "ename": "IndentationError",
          "evalue": "unexpected indent (4283674236.py, line 2)",
          "output_type": "error"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement object detection"
      ],
      "metadata": {
        "id": "iO8oOHy5-WQV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_video(video_file, model, categories, user_labels, base_dir, local_rank, max_frames):\n",
        "    device = torch.device(f\"cuda:{local_rank}\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-11T00:55:24.391556Z",
          "iopub.status.idle": "2024-08-11T00:55:24.39196Z",
          "shell.execute_reply.started": "2024-08-11T00:55:24.391787Z",
          "shell.execute_reply": "2024-08-11T00:55:24.391805Z"
        },
        "trusted": true,
        "id": "opsp3EA7-WQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_object_detection_model():\n",
        "    weights = FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT\n",
        "    model = fasterrcnn_resnet50_fpn_v2(weights=weights, box_score_thresh=0.9)\n",
        "    model.eval()\n",
        "    return model, weights.meta[\"categories\"]\n",
        "\n",
        "model, categories = get_object_detection_model()\n",
        "\n",
        "def detect_objects(image, model):\n",
        "    tensor = F.to_tensor(image).unsqueeze(0)\n",
        "    with torch.no_grad():\n",
        "        prediction = model(tensor)[0]\n",
        "    return prediction"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-11T00:55:24.393858Z",
          "iopub.status.idle": "2024-08-11T00:55:24.394256Z",
          "shell.execute_reply.started": "2024-08-11T00:55:24.394023Z",
          "shell.execute_reply": "2024-08-11T00:55:24.394037Z"
        },
        "trusted": true,
        "id": "38RmnoWL-WQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Label detected objects and save data"
      ],
      "metadata": {
        "id": "6Rtevv43-WQV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def label_and_save_objects(image, prediction, categories, user_labels, base_dir, idx, split):\n",
        "    img = Image.fromarray(image)\n",
        "    draw = ImageDraw.Draw(img)\n",
        "    font = ImageFont.truetype(\"/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf\", 20)\n",
        "\n",
        "    img_path = f\"{base_dir}/images/{split}/frame_{idx:04d}.jpg\"\n",
        "    label_path = f\"{base_dir}/labels/{split}/frame_{idx:04d}.txt\"\n",
        "\n",
        "    img.save(img_path)\n",
        "\n",
        "    with open(label_path, 'w') as f:\n",
        "        for box, label, score in zip(prediction[\"boxes\"], prediction[\"labels\"], prediction[\"scores\"]):\n",
        "            if score > 0.5:\n",
        "                box = box.tolist()\n",
        "                category = categories[label]\n",
        "                if category in user_labels:\n",
        "                    color = user_labels[category]\n",
        "                    draw.rectangle(box, outline=color, width=3)\n",
        "                    draw.text((box[0], box[1]), f\"{category}: {score:.2f}\", fill=color, font=font)\n",
        "\n",
        "                    # Convert box coordinates to YOLO format\n",
        "                    x_center = (box[0] + box[2]) / 2 / img.width\n",
        "                    y_center = (box[1] + box[3]) / 2 / img.height\n",
        "                    width = (box[2] - box[0]) / img.width\n",
        "                    height = (box[3] - box[1]) / img.height\n",
        "\n",
        "                    # Write label in YOLO format\n",
        "                    f.write(f\"{list(user_labels.keys()).index(category)} {x_center} {y_center} {width} {height}\\n\")\n",
        "\n",
        "    return np.array(img)\n",
        "\n",
        "# Get user input for labels and colors\n",
        "user_labels = {}\n",
        "while True:\n",
        "    category = input(\"Enter a category to label (or 'done' to finish): \")\n",
        "    if category.lower() == 'done':\n",
        "        break\n",
        "    color = input(f\"Enter a color for {category} (e.g., 'red', 'blue', 'green'): \")\n",
        "    user_labels[category] = color"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-11T00:57:21.933191Z",
          "iopub.execute_input": "2024-08-11T00:57:21.93416Z",
          "iopub.status.idle": "2024-08-11T00:58:06.33607Z",
          "shell.execute_reply.started": "2024-08-11T00:57:21.934124Z",
          "shell.execute_reply": "2024-08-11T00:58:06.335249Z"
        },
        "trusted": true,
        "id": "EHYJb4KY-WQV",
        "outputId": "5039ae3e-51b4-4656-b669-9ec5c95e8981"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdin",
          "text": "Enter a category to label (or 'done' to finish):  PLASTIC BOTTLES\nEnter a color for PLASTIC BOTTLES (e.g., 'red', 'blue', 'green'):  RED\nEnter a category to label (or 'done' to finish):  GLASS BOTTLES\nEnter a color for GLASS BOTTLES (e.g., 'red', 'blue', 'green'):  BLUE\nEnter a category to label (or 'done' to finish):  MILK CARTONS\nEnter a color for MILK CARTONS (e.g., 'red', 'blue', 'green'):  GREEN\nEnter a category to label (or 'done' to finish):  DONE\n"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Process video and create dataset\n"
      ],
      "metadata": {
        "id": "0S-Aq2xl-WQV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_video_and_create_dataset(frames, model, categories, user_labels, base_dir):\n",
        "    processed_frames = []\n",
        "\n",
        "    # Split frames into train and validation sets\n",
        "    train_frames, val_frames = train_test_split(frames, test_size=0.2, random_state=42)\n",
        "\n",
        "    for split, split_frames in [(\"train\", train_frames), (\"val\", val_frames)]:\n",
        "        for idx, frame in enumerate(split_frames):\n",
        "            prediction = detect_objects(frame, model)\n",
        "            labeled_frame = label_and_save_objects(frame, prediction, categories, user_labels, base_dir, idx, split)\n",
        "            processed_frames.append(labeled_frame)\n",
        "\n",
        "    return processed_frames\n",
        "\n",
        "processed_frames = process_video_and_create_dataset(frames, model, categories, user_labels, base_dir)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-11T00:58:49.715361Z",
          "iopub.execute_input": "2024-08-11T00:58:49.716369Z",
          "iopub.status.idle": "2024-08-11T00:58:49.755567Z",
          "shell.execute_reply.started": "2024-08-11T00:58:49.716334Z",
          "shell.execute_reply": "2024-08-11T00:58:49.754177Z"
        },
        "trusted": true,
        "id": "MUMB5IwE-WQW",
        "outputId": "b3f8166a-7230-4e68-dede-1ac7cc910311"
      },
      "execution_count": null,
      "outputs": [
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[22], line 15\u001b[0m\n\u001b[1;32m     11\u001b[0m             processed_frames\u001b[38;5;241m.\u001b[39mappend(labeled_frame)\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m processed_frames\n\u001b[0;32m---> 15\u001b[0m processed_frames \u001b[38;5;241m=\u001b[39m process_video_and_create_dataset(frames, model, categories, user_labels, \u001b[43mbase_dir\u001b[49m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'base_dir' is not defined"
          ],
          "ename": "NameError",
          "evalue": "name 'base_dir' is not defined",
          "output_type": "error"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Display results and create dataset info\n"
      ],
      "metadata": {
        "id": "KHskezZo-WQW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first processed frame\n",
        "from IPython.display import Image as IPImage\n",
        "from IPython.display import display\n",
        "\n",
        "first_frame = Image.fromarray(processed_frames[0])\n",
        "first_frame.save(\"first_frame.jpg\")\n",
        "display(IPImage(\"first_frame.jpg\"))\n",
        "\n",
        "# Create dataset info file\n",
        "def create_dataset_info(base_dir, user_labels):\n",
        "    with open(f\"{base_dir}/dataset_info.txt\", 'w') as f:\n",
        "        f.write(\"Dataset Information\\n\")\n",
        "        f.write(\"-------------------\\n\")\n",
        "        f.write(f\"Total frames: {len(processed_frames)}\\n\")\n",
        "        f.write(f\"Training frames: {len(os.listdir(f'{base_dir}/images/train'))}\\n\")\n",
        "        f.write(f\"Validation frames: {len(os.listdir(f'{base_dir}/images/val'))}\\n\")\n",
        "        f.write(\"\\nLabels:\\n\")\n",
        "        for idx, (category, color) in enumerate(user_labels.items()):\n",
        "            f.write(f\"{idx}: {category} ({color})\\n\")\n",
        "\n",
        "create_dataset_info(base_dir, user_labels)\n",
        "\n",
        "print(f\"Labeled dataset created at {base_dir}\")\n",
        "print(f\"Dataset information saved to {base_dir}/dataset_info.txt\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "tiqXZ5yU-WQW",
        "outputId": "b004e61d-be65-4b6a-eb6d-e448347d8f21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'processed_frames' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-801c8c0a0daa>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mfirst_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_frames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mfirst_frame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"first_frame.jpg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIPImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"first_frame.jpg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'processed_frames' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "to use this updated notebook now includes the following additional features:\n",
        "\n",
        "Creates separate directories for images and labels, both for training and validation sets.\n",
        "Splits the video frames into training and validation sets (80% train, 20% validation).\n",
        "Saves labeled images and corresponding label files in YOLO format.\n",
        "Creates a dataset information file with details about the number of frames and label categories.\n",
        "To use this notebook in Kaggle:\n",
        "\n",
        "Create a new notebook in your Kaggle environment.\n",
        "Copy and paste the code snippets into the appropriate cells.\n",
        "Upload your video file to the Kaggle dataset and update the video_path variable.\n",
        "Run the cells in order, providing input for the categories you want to label when prompted."
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-11T00:55:24.447554Z",
          "iopub.status.idle": "2024-08-11T00:55:24.448023Z",
          "shell.execute_reply.started": "2024-08-11T00:55:24.447793Z",
          "shell.execute_reply": "2024-08-11T00:55:24.447813Z"
        },
        "trusted": true,
        "id": "bPx2CQkK-WQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %% [markdown]\n",
        "# # Video Processing and Object Detection Dataset Creation\n",
        "#\n",
        "# This notebook processes video files to create a labeled dataset for object detection. It uses the Faster R-CNN model for object detection and allows users to specify categories and colors for labeling.\n",
        "\n",
        "# %% [markdown]\n",
        "# ## Import Required Libraries\n",
        "\n",
        "# %%\n",
        "import cv2\n",
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import os\n",
        "import shutil\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights\n",
        "from torchvision.transforms import functional as F\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import logging\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "# %% [markdown]\n",
        "# ## Set up logging\n",
        "\n",
        "# %%\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# %% [markdown]\n",
        "# ## Directory Creation and File Handling\n",
        "\n",
        "# %%\n",
        "def create_directories(base_dir):\n",
        "    dirs = [\n",
        "        f\"{base_dir}/images/train\",\n",
        "        f\"{base_dir}/images/val\",\n",
        "        f\"{base_dir}/labels/train\",\n",
        "        f\"{base_dir}/labels/val\"\n",
        "    ]\n",
        "    for dir in dirs:\n",
        "        os.makedirs(dir, exist_ok=True)\n",
        "    return base_dir\n",
        "\n",
        "def get_video_files(root_dir):\n",
        "    video_extensions = ['.mp4', '.avi', '.mov', '.mkv']\n",
        "    video_files = []\n",
        "    for root, _, files in os.walk(root_dir):\n",
        "        for file in files:\n",
        "            if any(file.lower().endswith(ext) for ext in video_extensions):\n",
        "                video_files.append(os.path.join(root, file))\n",
        "    return video_files\n",
        "\n",
        "# %% [markdown]\n",
        "# ## Video Loading and Processing\n",
        "\n",
        "# %%\n",
        "def load_video(video_path, max_frames=None):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        logging.error(f\"Could not open video file at {video_path}\")\n",
        "        return []\n",
        "\n",
        "    frames = []\n",
        "    frame_count = 0\n",
        "    with tqdm(total=int(cap.get(cv2.CAP_PROP_FRAME_COUNT)), desc=f\"Loading {os.path.basename(video_path)}\") as pbar:\n",
        "        while cap.isOpened():\n",
        "            ret, frame = cap.read()\n",
        "            if not ret or (max_frames and frame_count >= max_frames):\n",
        "                break\n",
        "            frames.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "            frame_count += 1\n",
        "            pbar.update(1)\n",
        "\n",
        "    cap.release()\n",
        "    logging.info(f\"Loaded {len(frames)} frames from {video_path}\")\n",
        "    return frames\n",
        "\n",
        "# %% [markdown]\n",
        "# ## Object Detection Model\n",
        "\n",
        "# %%\n",
        "def get_object_detection_model(device):\n",
        "    weights = FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT\n",
        "    model = fasterrcnn_resnet50_fpn_v2(weights=weights, box_score_thresh=0.9)\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    return model, weights.meta[\"categories\"]\n",
        "\n",
        "def detect_objects(image, model, device):\n",
        "    tensor = F.to_tensor(image).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        prediction = model(tensor)[0]\n",
        "    return {k: v.cpu() for k, v in prediction.items()}\n",
        "\n",
        "# %% [markdown]\n",
        "# ## Object Labeling and Saving\n",
        "\n",
        "# %%\n",
        "def label_and_save_objects(image, prediction, categories, user_labels, base_dir, idx, split):\n",
        "    img = Image.fromarray(image)\n",
        "    draw = ImageDraw.Draw(img)\n",
        "    font = ImageFont.load_default()\n",
        "\n",
        "    img_path = f\"{base_dir}/images/{split}/frame_{idx:06d}.jpg\"\n",
        "    label_path = f\"{base_dir}/labels/{split}/frame_{idx:06d}.txt\"\n",
        "\n",
        "    img.save(img_path)\n",
        "\n",
        "    with open(label_path, 'w') as f:\n",
        "        for box, label, score in zip(prediction[\"boxes\"], prediction[\"labels\"], prediction[\"scores\"]):\n",
        "            if score > 0.5:\n",
        "                box = box.tolist()\n",
        "                category = categories[label]\n",
        "                if category in user_labels:\n",
        "                    color = user_labels[category]\n",
        "                    draw.rectangle(box, outline=color, width=3)\n",
        "                    draw.text((box[0], box[1]), f\"{category}: {score:.2f}\", fill=color, font=font)\n",
        "\n",
        "                    x_center = (box[0] + box[2]) / 2 / img.width\n",
        "                    y_center = (box[1] + box[3]) / 2 / img.height\n",
        "                    width = (box[2] - box[0]) / img.width\n",
        "                    height = (box[3] - box[1]) / img.height\n",
        "\n",
        "                    f.write(f\"{list(user_labels.keys()).index(category)} {x_center} {y_center} {width} {height}\\n\")\n",
        "\n",
        "    return np.array(img)\n",
        "\n",
        "# %% [markdown]\n",
        "# ## Video Processing Functions\n",
        "\n",
        "# %%\n",
        "def process_video(video_file, model, categories, user_labels, base_dir, device, max_frames):\n",
        "    frames = load_video(video_file, max_frames)\n",
        "    processed_frames = []\n",
        "\n",
        "    for idx, frame in enumerate(tqdm(frames, desc=f\"Processing {os.path.basename(video_file)}\")):\n",
        "        prediction = detect_objects(frame, model, device)\n",
        "        labeled_frame = label_and_save_objects(frame, prediction, categories, user_labels, base_dir, idx, \"train\")\n",
        "        processed_frames.append(labeled_frame)\n",
        "\n",
        "    return processed_frames\n",
        "\n",
        "def process_videos_and_create_dataset(video_files, model, categories, user_labels, base_dir, device, max_frames, num_workers):\n",
        "    all_processed_frames = []\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
        "        future_to_video = {executor.submit(process_video, video, model, categories, user_labels, base_dir, device, max_frames): video for video in video_files}\n",
        "        for future in tqdm(as_completed(future_to_video), total=len(video_files), desc=\"Processing videos\"):\n",
        "            video = future_to_video[future]\n",
        "            try:\n",
        "                processed_frames = future.result()\n",
        "                all_processed_frames.extend(processed_frames)\n",
        "            except Exception as exc:\n",
        "                logging.error(f\"{video} generated an exception: {exc}\")\n",
        "\n",
        "    # Split frames into train and validation sets\n",
        "    train_frames, val_frames = train_test_split(all_processed_frames, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Move validation frames to val directory\n",
        "    for idx, frame in enumerate(val_frames):\n",
        "        src_img = f\"{base_dir}/images/train/frame_{idx:06d}.jpg\"\n",
        "        src_label = f\"{base_dir}/labels/train/frame_{idx:06d}.txt\"\n",
        "        dst_img = f\"{base_dir}/images/val/frame_{idx:06d}.jpg\"\n",
        "        dst_label = f\"{base_dir}/labels/val/frame_{idx:06d}.txt\"\n",
        "\n",
        "        shutil.move(src_img, dst_img)\n",
        "        shutil.move(src_label, dst_label)\n",
        "\n",
        "    return all_processed_frames\n",
        "\n",
        "# %% [markdown]\n",
        "# ## Dataset Information Creation\n",
        "\n",
        "# %%\n",
        "def create_dataset_info(base_dir, user_labels, processed_frames):\n",
        "    info = {\n",
        "        \"total_frames\": len(processed_frames),\n",
        "        \"training_frames\": len(os.listdir(f'{base_dir}/images/train')),\n",
        "        \"validation_frames\": len(os.listdir(f'{base_dir}/images/val')),\n",
        "        \"labels\": {idx: {\"category\": category, \"color\": color} for idx, (category, color) in enumerate(user_labels.items())}\n",
        "    }\n",
        "\n",
        "    with open(f\"{base_dir}/dataset_info.json\", 'w') as f:\n",
        "        json.dump(info, f, indent=4)\n",
        "\n",
        "    logging.info(f\"Dataset information saved to {base_dir}/dataset_info.json\")\n",
        "\n",
        "# %% [markdown]\n",
        "# ## Main Execution\n",
        "\n",
        "# %%\n",
        "def main(input_dir, output_dir, max_frames=None, num_workers=4):\n",
        "    base_dir = create_directories(output_dir)\n",
        "    video_files = get_video_files(input_dir)\n",
        "\n",
        "    if not video_files:\n",
        "        logging.error(\"No video files found in the specified directory and its subdirectories.\")\n",
        "        return\n",
        "\n",
        "    logging.info(f\"Found {len(video_files)} video files.\")\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model, categories = get_object_detection_model(device)\n",
        "\n",
        "    user_labels = {}\n",
        "    while True:\n",
        "        category = input(\"Enter a category to label (or 'done' to finish): \")\n",
        "        if category.lower() == 'done':\n",
        "            break\n",
        "        color = input(f\"Enter a color for {category} (e.g., 'red', 'blue', 'green'): \")\n",
        "        user_labels[category] = color\n",
        "\n",
        "    processed_frames = process_videos_and_create_dataset(video_files, model, categories, user_labels, base_dir, device, max_frames, num_workers)\n",
        "\n",
        "    if processed_frames:\n",
        "        create_dataset_info(base_dir, user_labels, processed_frames)\n",
        "        logging.info(f\"Labeled dataset created at {base_dir}\")\n",
        "    else:\n",
        "        logging.warning(\"No frames were processed. Check if the video files are valid and contain frames.\")\n",
        "\n",
        "# %% [markdown]\n",
        "# ## Run the Script\n",
        "#\n",
        "# To run the script, set the `input_dir` and `output_dir` variables below, and then execute this cell.\n",
        "\n",
        "# %%\n",
        "input_dir = \"/kaggle/input/cdsset\"  # Replace with your input directory\n",
        "input_dir = \"/kaggle/input/bottle-videos\"\n",
        "output_dir = \"/kaggle/working/\"  # Replace with your output directory\n",
        "max_frames = None # Set to None to process all frames\n",
        "num_workers = 4  # Adjust based on your system's capabilities\n",
        "\n",
        "main(input_dir, output_dir, max_frames, num_workers)\n",
        "\n",
        "# %% [markdown]\n",
        "# ## Conclusion\n",
        "#\n",
        "# This notebook processes video files to create a labeled dataset for object detection. It uses the Faster R-CNN model for object detection and allows users to specify categories and colors for labeling. The resulting dataset is split into training and validation sets, and a JSON file with dataset information is created."
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-11T00:55:24.449367Z",
          "iopub.status.idle": "2024-08-11T00:55:24.449825Z",
          "shell.execute_reply.started": "2024-08-11T00:55:24.449587Z",
          "shell.execute_reply": "2024-08-11T00:55:24.449613Z"
        },
        "trusted": true,
        "id": "9ELMtY26-WQW",
        "outputId": "03d61b8a-849b-4daf-ed83-e9fef39ec9ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:No video files found in the specified directory and its subdirectories.\n"
          ]
        }
      ]
    }
  ]
}